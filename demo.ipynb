{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we replicate the methodology from [Wilcox et al (2023)](http://www.colinphillips.net/wp-content/uploads/2023/02/wilcox2022.pdf), evaluating a suite of LSTM & Transformer language models on various syntactic constructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of experiments the paper reports is on filler-gap dependencies. These are based on Example 5a from the paper, using the 2x2 design described in section 3 (wh/that x +gap/-gap). In this example, `the businessman` is in the gap position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_gap = \"I know who without thinking showed the slides to the guests after lunch.\"\n",
    "that_gap = \"I know that without thinking showed the slides to the guests after lunch.\"\n",
    "wh_no_gap = \"I know who without thinking the businessman showed the slides to the guests after lunch.\"\n",
    "that_no_gap = \"I know that without thinking the businessman showed the slides to the guests after lunch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we compute GPT2 surprisal. It's the easiest to use out of the box, though we may want to use something like [LMZoo](https://github.com/cpllab/lm-zoo) if we can get it to work in Python (I tried this prior to getting GPT2 working), or if our data format can work with its CLI. Wilcox et al compare the surprisal of the token after the gap (the critical region) in grammatical and ungrammatical conditions. For the examples above, it is `showed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minicons import scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model = scorer.IncrementalLMScorer(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wh_effect(model, wh_sentence, that_sentence, crit_region):\n",
    "    surprisals = model.token_score([wh_sentence, that_sentence], surprisal = True, base_two = True)\n",
    "    critical_surprisals = [token_score[1] for sentence in surprisals for token_score in sentence\n",
    "                           if token_score[0] == crit_region]\n",
    "    return critical_surprisals[0] - critical_surprisals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.653267860412598"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_wh_effect(model, wh_gap, that_gap, \"showed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5451717376708984"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_wh_effect(model, wh_no_gap, that_no_gap, \"showed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in line with the result in Figure 4 of the paper: the wh-effect is negative in the +gap condition and positive in the -gap condition. This means the word after the gap is more surprising in the ungrammatical cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-gen-env",
   "language": "python",
   "name": "lm-gen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
